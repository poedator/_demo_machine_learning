{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB review dataset excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup # for HTML cleanup\n",
    "# import unicodedata # to deal with accented characters\n",
    "import spacy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "print( 'tf version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NLP packets to import:\n",
    "conda install nltk, bs4, gensim, spacy, pymorphy2, statsmodels\n",
    "conda install -c conda-forge transformers optuna\n",
    "\n",
    "pip install pycontractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# limit GPU mem growth (source: https://www.tensorflow.org/guide/gpu)\n",
    "tf.config.experimental.set_memory_growth(\n",
    "    tf.config.experimental.list_physical_devices('GPU')[0], True)\n",
    "\n",
    "# expand notebook to full window width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'u:\\\\bigdata\\\\_datasets\\\\NLP\\\\IMDB Dataset.csv.zip'\n",
    "df = pd.read_csv(dataset_path, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    25000\n",
       "positive    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max review length: 13704\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of strings lengths'\n",
    "print(\"max review length:\", df.review.str.len().max())\n",
    "df.review.str.len().hist(bins=100, figsize = (12,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode sentiment as 0 / 1\n",
    "if df.sentiment.dtype == 'O':\n",
    "    df.sentiment = (df.sentiment=='positive').astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "# remove HTML tags\n",
    "%time df['review'] = df.review.apply(lambda x: BeautifulSoup(x).get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with accented characters\n",
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "# to test:\n",
    "# remove_accented_chars('Sómě Áccěntěd těxt. Some words such as résumé, café, prótest, divorcé, coördinate, exposé, latté.')\n",
    "\n",
    "# %time df['rev'] = df.review.apply(remove_accented_chars)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# contractions ======================================\n",
    "from contractions import CONTRACTION_MAP # from contractions.py\n",
    "import re \n",
    "# function to expand contractions\n",
    "def expand_contractions(text, map=CONTRACTION_MAP):\n",
    "    pattern = re.compile('({})'.format('|'.join(map.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    def get_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded = map.get(match) if map.get(match) else map.get(match.lower())\n",
    "        expanded = first_char+expanded[1:]\n",
    "        return expanded \n",
    "    new_text = pattern.sub(get_match, text)\n",
    "    new_text = re.sub(\"'\", \"\", new_text)\n",
    "    return new_text\n",
    "# call function \n",
    "expand_contractions(\"Y’all i’d contractions you’re expanded don’t think.\")\n",
    "\n",
    "# contractions ======================================\n",
    "\n",
    "# imports\n",
    "from pycontractions import Contractions\n",
    "cont = Contractions(kv_model=model)\n",
    "cont.load_models()\n",
    "# function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 160 ms\n"
     ]
    }
   ],
   "source": [
    "# remove contracted endings 'll 're 's 'd  etc\n",
    "%time df['rev'] = df.review.str.replace(r\"\\'[a-zA-Z]+\\s\",\" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.87 s\n"
     ]
    }
   ],
   "source": [
    "# remove non-letter characters\n",
    "# remove_pattern = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' # longer keep list\n",
    "remove_pattern = r'[^a-zA-Z]'  # just letters\n",
    "\n",
    "%time df.rev = df.rev.str.replace(remove_pattern,\" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 86 ms\n"
     ]
    }
   ],
   "source": [
    "# to lowercase\n",
    "%time df.rev = df.rev.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "# len(stopword_list)\n",
    "\n",
    "def remove_stopwords(x):  \n",
    "    return' '.join([w for w in tokenizer.tokenize(x) if not w in stopword_list])\n",
    "\n",
    "%time df.rev =  df.rev.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZE\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize(x):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokenizer.tokenize(x)])\n",
    "\n",
    "%time df.rev = df.rev.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FIX UNICODE TEXT\n",
    "from ftfy import fix_text\n",
    "print(fix_text(u'\\001\\033[36;44mI&#x92;m blue, da ba dee da ba doo&#133;\\033[0m', normalization='NFKC'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# SPACY for lemmatization, tokenization\n",
    "\n",
    "# imports\n",
    "import spacy\n",
    "nlp = spacy.lang.en.English()\n",
    "# nlp = spacy.load('en',parse=True,tag=True, entity=True)\n",
    "# function to remove special characters\n",
    "def get_lem(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "# call function\n",
    "get_lem(\"we are eating and swimming ; we have been eating and swimming ; he eats and swims ; he ate and swam \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34242</th>\n",
       "      <td>This movie is just a lot of fun. I've seen it ...</td>\n",
       "      <td>1</td>\n",
       "      <td>movie lot fun seen couple time always somethin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18593</th>\n",
       "      <td>A good friend of mine one said: \"A monkey is f...</td>\n",
       "      <td>0</td>\n",
       "      <td>good friend mine one said monkey funny anytime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>This utterly dull, senseless, pointless, spiri...</td>\n",
       "      <td>0</td>\n",
       "      <td>utterly dull senseless pointless spiritless du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4794</th>\n",
       "      <td>The acting was very sub-par, You had Costas Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>acting sub par costa mandalar acting like trip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32778</th>\n",
       "      <td>One Stinko of a movie featuring a shopworn plo...</td>\n",
       "      <td>0</td>\n",
       "      <td>one stinko movie featuring shopworn plot kind ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment  \\\n",
       "34242  This movie is just a lot of fun. I've seen it ...          1   \n",
       "18593  A good friend of mine one said: \"A monkey is f...          0   \n",
       "6051   This utterly dull, senseless, pointless, spiri...          0   \n",
       "4794   The acting was very sub-par, You had Costas Ma...          0   \n",
       "32778  One Stinko of a movie featuring a shopworn plo...          0   \n",
       "\n",
       "                                                     rev  \n",
       "34242  movie lot fun seen couple time always somethin...  \n",
       "18593  good friend mine one said monkey funny anytime...  \n",
       "6051   utterly dull senseless pointless spiritless du...  \n",
       "4794   acting sub par costa mandalar acting like trip...  \n",
       "32778  one stinko movie featuring shopworn plot kind ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d09b216749445a9950bbbae3b286bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 91574\n"
     ]
    }
   ],
   "source": [
    "# Counting unique words\n",
    "from collections import Counter\n",
    "unique_words = Counter()\n",
    "for review in tqdm(df.rev.values):\n",
    "    unique_words.update(review.split())\n",
    "print (f\"Unique words: {len(unique_words)}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of words in corpus by num of occurences\n",
      "1     0.397995\n",
      "2     0.116703\n",
      "3     0.063457\n",
      "4     0.042872\n",
      "5     0.031210\n",
      "6     0.024068\n",
      "7     0.020715\n",
      "8     0.016904\n",
      "9     0.015321\n",
      "10    0.012023\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# count rare words\n",
    "print(\"percent of words in corpus by num of occurences\")\n",
    "print(pd.Series(unique_words.values()).value_counts().head(10)/len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of words in corpus by length\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.027781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.108797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.153417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.160810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.143054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.118331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.085799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words\n",
       "index          \n",
       "1      0.000273\n",
       "2      0.005307\n",
       "3      0.027781\n",
       "4      0.064516\n",
       "5      0.108797\n",
       "6      0.153417\n",
       "7      0.160810\n",
       "8      0.143054\n",
       "9      0.118331\n",
       "10     0.085799"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count words by length\n",
    "print(\"percent of words in corpus by length\")\n",
    "pd.Series(unique_words.keys(), name=\"words\").str.len().value_counts().to_frame().reset_index().\\\n",
    "    sort_values(by='index').head(10).set_index('index')/len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words to be used for regression: 24508\n"
     ]
    }
   ],
   "source": [
    "# remove short and infrequent words\n",
    "min_occurencies = 10\n",
    "min_word_len = 3\n",
    "\n",
    "my_vocab = {k:v for k, v in unique_words.items() if v>=min_occurencies and len(k)>= min_word_len}\n",
    "\n",
    "my_vocab = {k: v for k, v in sorted(my_vocab.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "vocab_size = len(my_vocab)\n",
    "print (f\"Words to be used for regression: {vocab_size}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes (with limited vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4 s\n"
     ]
    }
   ],
   "source": [
    "# preparing data for naive Bayes\n",
    "vectorizer = CountVectorizer(vocabulary = list(my_vocab.keys()))\n",
    "%time Xv = vectorizer.fit_transform(df.rev.values) \n",
    "y = df['sentiment'].values\n",
    "\n",
    "nb_X_train, nb_X_val, nb_y_train, nb_y_val = train_test_split(\n",
    "    Xv, df['sentiment'].values, \n",
    "    test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes baseline accuracy score: 0.857\n"
     ]
    }
   ],
   "source": [
    "# run Naïve Bayes model\n",
    "nb_model = MultinomialNB().fit(nb_X_train, nb_y_train)\n",
    "\n",
    "nb_y_pred = nb_model.predict(nb_X_val)\n",
    "\n",
    "nb_score = accuracy_score(nb_y_val, nb_y_pred)\n",
    "print (f\"Naïve Bayes baseline accuracy score: {nb_score:.4}\")\n",
    "\n",
    "# Note: without vocab limit NB scores ~0.861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing feed for NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentences tokenized with 24508 words vocab\n",
      "Max cleaned review length: 9103; limiting/padding review to 2000 words\n",
      "(45000, 2000) (5000, 2000)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=len(list(my_vocab)))\n",
    "tokenizer.fit_on_texts(my_vocab.keys())\n",
    "X_tokenized = tokenizer.texts_to_sequences(df.rev)\n",
    "print (f\"Input sentences tokenized with {tokenizer.get_config()['num_words']} words vocab\")\n",
    "# y = df.sentiment.values\n",
    "\n",
    "maxlen = 2000\n",
    "len_max = df.rev.str.len().max()\n",
    "print (f\"Max cleaned review length: {len_max}; limiting/padding review to {maxlen} words\")\n",
    "X_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_tokenized, maxlen=maxlen, padding='pre',)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_padded, df['sentiment'].values, \n",
    "    test_size=0.1, random_state=42, stratify=y)\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## simple 2-layer LSTM model (emb 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_17 (Embedding)     (None, None, 32)          784256    \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, None, 128)         82432     \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 922,798\n",
      "Trainable params: 922,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# simple LSTM model\n",
    "embed_dim = 32\n",
    "\n",
    "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
    "x = layers.Embedding(tokenizer.get_config()['num_words'], embed_dim)(inputs)\n",
    "x = layers.LSTM(128, dropout=0.5, recurrent_dropout=0.0,\n",
    "                return_sequences=True)(x)\n",
    "x = layers.LSTM(64, dropout=0.5, recurrent_dropout=0.0,\n",
    "                return_sequences=False)(x)\n",
    "x = layers.Dense(100, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model_e32 = tf.keras.Model(inputs, outputs)\n",
    "model_e32.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.002),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_e32.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "704/704 [==============================] - 113s 156ms/step - loss: 0.4775 - accuracy: 0.7520 - val_loss: 0.2814 - val_accuracy: 0.8834\n",
      "Epoch 2/6\n",
      "704/704 [==============================] - 109s 154ms/step - loss: 0.2322 - accuracy: 0.9129 - val_loss: 0.2702 - val_accuracy: 0.8962\n",
      "Epoch 3/6\n",
      "704/704 [==============================] - 109s 155ms/step - loss: 0.1725 - accuracy: 0.9387 - val_loss: 0.2997 - val_accuracy: 0.8886\n",
      "Epoch 4/6\n",
      "704/704 [==============================] - 109s 155ms/step - loss: 0.1397 - accuracy: 0.9503 - val_loss: 0.2839 - val_accuracy: 0.8896\n"
     ]
    }
   ],
   "source": [
    "history_e32 = model_e32.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=6, batch_size=64, verbose=1, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM model accuracy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "print (f\"LSTM model accuracy: {max(model_e32.history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM model\n",
    "https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, None, 300)         7352400   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 128)         186880    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 7,651,198\n",
      "Trainable params: 7,651,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input for variable-length sequences of integers\n",
    "embed_dim = 300\n",
    "\n",
    "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(tokenizer.get_config()['num_words'], embed_dim)(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Add a classifier\n",
    "x = layers.Dense(100, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs, outputs)\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "704/704 [==============================] - 239s 332ms/step - loss: 0.4728 - accuracy: 0.7855 - val_loss: 0.3355 - val_accuracy: 0.8714\n",
      "Epoch 2/6\n",
      "704/704 [==============================] - 232s 329ms/step - loss: 0.2610 - accuracy: 0.9019 - val_loss: 0.2633 - val_accuracy: 0.8932\n",
      "Epoch 3/6\n",
      "704/704 [==============================] - 233s 331ms/step - loss: 0.1565 - accuracy: 0.9469 - val_loss: 0.3118 - val_accuracy: 0.8866\n",
      "Epoch 4/6\n",
      "704/704 [==============================] - 233s 331ms/step - loss: 0.0852 - accuracy: 0.9735 - val_loss: 0.3130 - val_accuracy: 0.8752\n"
     ]
    }
   ],
   "source": [
    "model_3.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.005),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history_3 = model_3.fit(X_train, y_train, \n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=6, batch_size=64, verbose=1, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional LSTM model accuracy: 0.8932\n"
     ]
    }
   ],
   "source": [
    "# y_pred = model_3.predict(X_val)\n",
    "# score = accuracy_score(y_val, y_pred.argmax(axis=1))\n",
    "# model_3.history.history\n",
    "print (f\"Bidirectional LSTM model accuracy: {max(history_3.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer\n",
    "based on https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model_t = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 2000)]            0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 2000, 32)          848256    \n",
      "_________________________________________________________________\n",
      "transformer_block_1 (Transfo (None, 2000, 32)          10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 859,614\n",
      "Trainable params: 859,614\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_t.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1407/1407 [==============================] - 295s 208ms/step - loss: 0.6819 - accuracy: 0.5205 - val_loss: 0.3261 - val_accuracy: 0.8602\n",
      "Epoch 2/6\n",
      "1407/1407 [==============================] - 291s 207ms/step - loss: 0.2575 - accuracy: 0.8966 - val_loss: 0.2421 - val_accuracy: 0.9036\n",
      "Epoch 3/6\n",
      "1407/1407 [==============================] - 290s 206ms/step - loss: 0.1777 - accuracy: 0.9352 - val_loss: 0.2516 - val_accuracy: 0.8986\n",
      "Epoch 4/6\n",
      "1407/1407 [==============================] - 290s 206ms/step - loss: 0.1239 - accuracy: 0.9573 - val_loss: 0.3137 - val_accuracy: 0.8944\n"
     ]
    }
   ],
   "source": [
    "history_t = model_t.fit(X_train, y_train, \n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=6, batch_size=32, verbose=1, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer model accuracy: 0.9036\n"
     ]
    }
   ],
   "source": [
    "# y_pred = model_3.predict(X_val)\n",
    "# score = accuracy_score(y_val, y_pred.argmax(axis=1))\n",
    "# model_3.history.history\n",
    "print (f\"Transformer model accuracy: {max(history_t.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prereq: install Hugging Face transformers (conda has it)\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'transformers' from 'c:\\\\conda\\\\envs\\\\cuda11\\\\lib\\\\site-packages\\\\transformers\\\\__init__.py'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "b_tokenizer_c = transformers.BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_tensor_slices((df.rev, df.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = tf.data.experimental.CsvDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-0435626e116c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m gg = transformers.glue_convert_examples_to_features(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mexamples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mrpc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     tokenizer=b_tokenizer)\n",
      "\u001b[1;32mc:\\conda\\envs\\cuda11\\lib\\site-packages\\transformers\\data\\processors\\glue.py\u001b[0m in \u001b[0;36mglue_convert_examples_to_features\u001b[1;34m(examples, tokenizer, max_length, task, label_list, output_mode)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"When calling glue_convert_examples_to_features from TF, the task parameter is required.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_tf_glue_convert_examples_to_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     return _glue_convert_examples_to_features(\n\u001b[0;32m     73\u001b[0m         \u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\conda\\envs\\cuda11\\lib\\site-packages\\transformers\\data\\processors\\glue.py\u001b[0m in \u001b[0;36m_tf_glue_convert_examples_to_features\u001b[1;34m(examples, tokenizer, task, max_length)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \"\"\"\n\u001b[0;32m     90\u001b[0m         \u001b[0mprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglue_processors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfds_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_example_from_tensor_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglue_convert_examples_to_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mlabel_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sts-b\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\conda\\envs\\cuda11\\lib\\site-packages\\transformers\\data\\processors\\glue.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \"\"\"\n\u001b[0;32m     90\u001b[0m         \u001b[0mprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglue_processors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfds_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_example_from_tensor_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglue_convert_examples_to_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mlabel_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sts-b\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\conda\\envs\\cuda11\\lib\\site-packages\\transformers\\data\\processors\\glue.py\u001b[0m in \u001b[0;36mget_example_from_tensor_dict\u001b[1;34m(self, tensor_dict)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;34m\"\"\"See base class.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         return InputExample(\n\u001b[1;32m--> 180\u001b[1;33m             \u001b[0mtensor_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"idx\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m             \u001b[0mtensor_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentence1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mtensor_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentence2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "gg = transformers.glue_convert_examples_to_features(\n",
    "    examples=ds_train.prefetch(1000), task='mrpc',\n",
    "    tokenizer=b_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train.prefetch(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from C:\\Users\\rs\\tensorflow_datasets\\glue\\mrpc\\1.0.0\n",
      "INFO:absl:Reusing dataset glue (C:\\Users\\rs\\tensorflow_datasets\\glue\\mrpc\\1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from C:\\Users\\rs\\tensorflow_datasets\\glue\\mrpc\\1.0.0\n"
     ]
    }
   ],
   "source": [
    "data = tfds.load('glue/mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = transformers.glue_convert_examples_to_features(\n",
    "    examples=df.rev[:10].values, label_list = df.sentiment[:10].values,\n",
    "    tokenizer=b_tokenizer_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttributeError                            Traceback (most recent call last)\n",
    "<ipython-input-52-c53eee72a7c2> in <module>\n",
    "----> 1 gg = transformers.glue_convert_examples_to_features(\n",
    "      2     examples=df.rev[:10],\n",
    "      3     tokenizer=b_tokenizer_c)\n",
    "\n",
    "c:\\conda\\envs\\cuda11\\lib\\site-packages\\transformers\\data\\processors\\glue.py in glue_convert_examples_to_features(examples, tokenizer, max_length, task, label_list, output_mode)\n",
    "     70             raise ValueError(\"When calling glue_convert_examples_to_features from TF, the task parameter is required.\")\n",
    "     71         return _tf_glue_convert_examples_to_features(examples, tokenizer, max_length=max_length, task=task)\n",
    "---> 72     return _glue_convert_examples_to_features(\n",
    "     73         examples, tokenizer, max_length=max_length, task=task, label_list=label_list, output_mode=output_mode\n",
    "     74     )\n",
    "\n",
    "c:\\conda\\envs\\cuda11\\lib\\site-packages\\transformers\\data\\processors\\glue.py in _glue_convert_examples_to_features(examples, tokenizer, max_length, task, label_list, output_mode)\n",
    "    117 ):\n",
    "    118     if max_length is None:\n",
    "--> 119         max_length = tokenizer.max_len\n",
    "    120 \n",
    "    121     if task is not None:\n",
    "\n",
    "AttributeError: 'BertTokenizer' object has no attribute 'max_len'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = 'this is a sample text'\n",
    "str_list = df.loc[:2,'rev'].values.tolist()\n",
    "X_bt = b_tokenizer.batch_encode_plus(df.rev.values.tolist(), \n",
    "                                     max_length=maxlen,\n",
    "                                     truncation=True,\n",
    "                                     padding=True,\n",
    "                                     add_special_tokens=True,\n",
    "                                     return_attention_mask=True,\n",
    "                                     return_token_type_ids=True,\n",
    "                                     return_tensors=\"tf\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50000, 1669])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bt['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_bt, df['sentiment'].values, \n",
    "    test_size=0.1, random_state=42, stratify=y)\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X_bt = [b_tokenizer.encode(str_, max_length=maxlen, truncation=True) for str_ in df.rev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model = tf.keras.models.load_model(\n",
    "    \"bert_mlm_imdb.h5\",\n",
    "#     custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    ")\n",
    "pretrained_bert_model = tf.keras.Model(\n",
    "    mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert-2d attempt\n",
    "ref Kaggle bert example: https://www.kaggle.com/viroviro/detecting-disaster-tweets-fine-tuning-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from transformers import AutoConfig\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "# The number of labels of the target variable\n",
    "LABELS_NUMBER = 2\n",
    "\n",
    "# The max lenght of text can be up to 512 for BERT\n",
    "MAX_LENGHT = 256\n",
    "\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS_NUMBER = 1\n",
    "\n",
    "N_PREDICTIONS_TO_SHOW = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 30522\n",
      "Some tokens of the vocabulary: ['knight', 'lap', 'survey', 'ma', '##ow', 'noise', 'billy', '##ium', 'shooting', 'guide']\n"
     ]
    }
   ],
   "source": [
    "# Get the Bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME, \n",
    "                                          do_lower_case=True)\n",
    "\n",
    "vocabulary = tokenizer.get_vocab()\n",
    "print(f'Size of the vocabulary: {len(vocabulary)}')\n",
    "print(f'Some tokens of the vocabulary: {list(vocabulary.keys())[5000:5010]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize and prepare a sequence for the model. It tokenizes the text sequence\n",
    "    adding special tokens ([CLS], [SEP]), padding  to the max length and truncate \n",
    "    reviews longer than the max length.\n",
    "    Return the token IDs, the segment IDs and the mask IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    prepared_sequence = tokenizer.encode_plus(\n",
    "                            text, \n",
    "                            add_special_tokens = True, \n",
    "                            max_length = MAX_LENGHT, \n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = True,\n",
    "                            truncation = True,\n",
    "                            )\n",
    "    return prepared_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence:   Semantic Similarity is the task of determining how similar two sentences are.\n",
      "Keys:            dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "Tokens:          ['[CLS]', 'semantic', 'similarity', 'is', 'the', 'task', 'of', 'determining', 'how', 'similar', 'two', 'sentences', 'are', '.', '[SEP]', '[PAD]']\n",
      "Token IDs:       [101, 21641, 14402, 2003, 1996, 4708, 1997, 12515, 2129, 2714, 2048, 11746]\n",
      "Segment IDs:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Mask IDs         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Input dimension: 256\n"
     ]
    }
   ],
   "source": [
    "# Prepare a test sentence\n",
    "test_sentence = 'Semantic Similarity is the task of determining how similar two sentences are.'\n",
    "test_sentence_encoded = prepare_sequence(test_sentence, tokenizer)\n",
    "token_ids = test_sentence_encoded[\"input_ids\"]\n",
    "print(f'Test sentence:   {test_sentence}')\n",
    "print(f'Keys:            {test_sentence_encoded.keys()}')\n",
    "print(f'Tokens:          {tokenizer.convert_ids_to_tokens(token_ids)[:16]}')\n",
    "print(f'Token IDs:       {token_ids[:12]}')\n",
    "print(f'Segment IDs:     {test_sentence_encoded[\"token_type_ids\"][:16]}')\n",
    "print(f'Mask IDs         {test_sentence_encoded[\"attention_mask\"][:16]}')\n",
    "print(f'Input dimension: {len(token_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    \"\"\"\n",
    "    Map to the expected input to TFBertForSequenceClassification.\n",
    "    \"\"\"\n",
    "    mapped_example = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "    }\n",
    "    return mapped_example, label \n",
    "\n",
    "def encode_examples(texts, labels):\n",
    "    \"\"\"\n",
    "    Prepare all sequences of text and build TF dataset.\n",
    "    \"\"\"\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "        \n",
    "    for text, label in tqdm(zip(texts, labels), total=len(labels)):\n",
    "\n",
    "        bert_input = prepare_sequence(text, tokenizer)\n",
    "\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "\n",
    "    # Create TF dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (input_ids_list, attention_mask_list, token_type_ids_list,\n",
    "         label_list)\n",
    "    )\n",
    "    # Map to the expected input to TFBertForSequenceClassification\n",
    "    dataset_mapped = dataset.map(map_example_to_dict)\n",
    "    return dataset_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset for training and test\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.rev, df.sentiment,\n",
    "                                                  test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number examples in training dataset: 45000\n",
      "Number of positive examples in training dataset: 22481\n",
      "Number of negative examples in training dataset: 22519\n"
     ]
    }
   ],
   "source": [
    "n_training_examples = X_train.shape[0]\n",
    "n_positive_training_examples = y_train.value_counts()[1]\n",
    "n_negative_training_examples = y_train.value_counts()[0]\n",
    "print(f'Number examples in training dataset: {n_training_examples}')\n",
    "print(f'Number of positive examples in training dataset: {n_positive_training_examples}')\n",
    "print(f'Number of negative examples in training dataset: {n_negative_training_examples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91e77c129474877ab69a537fffd8804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare sequences of text and build TF train dataset\n",
    "ds_train_encoded = encode_examples(X_train, y_train).shuffle(10000).batch(BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a754101a654daa8a17985dbd4729b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare sequences of text and build TF validation dataset\n",
    "ds_val_encoded = encode_examples(X_val, y_val).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    # Define the configuration of the model\n",
    "    config = AutoConfig.from_pretrained(PRETRAINED_MODEL_NAME,\n",
    "                                        hidden_dropout_prob=0.2,\n",
    "                                        num_labels=LABELS_NUMBER)\n",
    "    # Model initialization\n",
    "    model = TFBertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, \n",
    "                                                            config=config)c\n",
    "    return model\n",
    "\n",
    "# Model initialization\n",
    "model = get_model()\n",
    "\n",
    "# Define the optimizer, the loss function and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.3637 - accuracy: 0.8338WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "7500/7500 [==============================] - 2396s 318ms/step - loss: 0.3637 - accuracy: 0.8338 - val_loss: 0.2312 - val_accuracy: 0.9078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fe1a91c310>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(ds_train_encoded, epochs=EPOCHS_NUMBER, validation_data=ds_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7500/7500 [==============================] - 2352s 314ms/step - loss: 0.0368 - accuracy: 0.9877 - val_loss: 0.3661 - val_accuracy: 0.8980\n",
      "Epoch 2/2\n",
      "7500/7500 [==============================] - 2354s 314ms/step - loss: 0.0318 - accuracy: 0.9899 - val_loss: 0.4563 - val_accuracy: 0.9038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fe1af05d60>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train_encoded, epochs=2, validation_data=ds_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7500/7500 [==============================] - 2355s 314ms/step - loss: 0.0298 - accuracy: 0.9908 - val_loss: 0.4844 - val_accuracy: 0.9052\n",
      "Epoch 2/3\n",
      "3370/7500 [============>.................] - ETA: 20:55 - loss: 0.0354 - accuracy: 0.9902"
     ]
    }
   ],
   "source": [
    "model.fit(ds_train_encoded, epochs=3, validation_data=ds_val_encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
