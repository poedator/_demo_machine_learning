{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ №6\n",
    "\n",
    "**Otus Neural Networks. 02-2020**\n",
    "\n",
    "**Тема: Практическое занятие на PyTorch. Генерация Википедии. Использование Torchtext**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе решения были применены следующие действия:\n",
    "\n",
    "    - использованы инструменты модуля `torchtext`\n",
    "        - `ReversibleField` для хранения элементов текста, со встроенным декодером\n",
    "        - `Example` для хранения используемых текстов \n",
    "        - `Dataset` - структура, объединяющая Example и Field\n",
    "        - `BPTTIterator` итератор батчей для обучения и тестирования, со сдвигом Targets на единицу - \n",
    "            создан для построения языковых моделей.\n",
    "    \n",
    "    - соответственно изменены функции обучения и тестирования\n",
    "            \n",
    "    - тексты очищены от служебных меток и редких символов.\n",
    "    \n",
    "    - модель переведена на использование ускорителя системы `CUDA`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт и ключевые параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs  # to fix encoding problems\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.2.0, device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Torch version: {torch.__version__}, device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, NestedField, Example, Dataset, ReversibleField\n",
    "from torchtext.data import Iterator, BPTTIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "train_log_interval = 500\n",
    "eval_batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text  (t0):\n",
    "    \"\"\"\n",
    "    removes non-latin characters,\n",
    "    compresses spaces\n",
    "    \"\"\"\n",
    "    t1 = t0.replace('<unk>', '')\n",
    "    t1 = t1.replace('\\n', ' ')\n",
    "\n",
    "    replacement = ' '\n",
    "    valid_chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz '-.,\"\n",
    "    t2 = ''.join([c if c in valid_chars else replacement for c in t1])\n",
    "    \n",
    "    t2 = re.sub(r'\\s+', ' ', t2)\n",
    "    t2 = re.sub(r'\\s([.,])', r'\\1', t2)  # fix space before comma/period\n",
    "    # t2 = t2.lower()\n",
    "    return t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading texts:\n",
      " train: loaded 10,780,437 chars, kept  9,726,465 chars, ratio:90.2%\n",
      " valid: loaded  1,120,192 chars, kept    975,304 chars, ratio:87.1%\n",
      "  test: loaded  1,255,018 chars, kept  1,078,111 chars, ratio:85.9%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Загрузка данных\"\"\"\n",
    "data_file_names = ['train', 'valid', 'test']\n",
    "path = '.\\\\wikitext\\\\'\n",
    "\n",
    "texts = []\n",
    "print ('Loading texts:')\n",
    "for d in data_file_names:\n",
    "    with codecs.open(os.path.join(path, f'{d}.txt'), \"r\", \"utf_8_sig\" ) as f:\n",
    "        text = f.read() # was f.readlines()\n",
    "        print (f\"{d:>6}: loaded {len(text):>10,} chars,\", end = '')\n",
    "        text2 = cleanup_text(text)\n",
    "        print (f\" kept {len(text2):>10,} chars, ratio:{len(text2)/len(text)*100:.1f}%\")\n",
    "        texts.append(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 105 ms\n"
     ]
    }
   ],
   "source": [
    "# Создаём датасеты\n",
    "text_field = ReversibleField (lower=False, tokenize=list, use_vocab=True, unk_token='_')\n",
    "fields = [('text', text_field)]\n",
    "%time examples = [Example.fromlist([text], fields) for text in texts]  # 3.6s\n",
    "\n",
    "datasets = [Dataset([example], fields, ) for example in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 59 [(' ', 1659076), ('e', 950936), ('t', 666909), ('a', 649233), ('n', 565603)]\n"
     ]
    }
   ],
   "source": [
    "# Строим словарь\n",
    "text_field.build_vocab(datasets[0], min_freq=100) \n",
    "ntokens  = len(text_field.vocab)\n",
    "\n",
    "print (len(text_field.vocab.freqs.items()), len(text_field.vocab),\n",
    "       text_field.vocab.freqs.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём итераторы\n",
    "train_iter = BPTTIterator.splits(datasets[:1],\n",
    "                                 batch_size=train_batch_size, bptt_len=sequence_length,\n",
    "                                 sort_key=len, shuffle=True, device=device)[0]\n",
    "\n",
    "val_iter, test_iter = BPTTIterator.splits(datasets[1:],\n",
    "                                          batch_size=eval_batch_size,\n",
    "                                          bptt_len=sequence_length,\n",
    "                                          sort_key=len,\n",
    "                                          shuffle=False, device=device,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели и функции для экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    global output, targets, data, cnt\n",
    "    data_size = 0\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, batch in enumerate(iter(data_loader)):\n",
    "        data = batch.text\n",
    "        targets= batch.target.flatten()\n",
    "        output, hidden = model(data)\n",
    "        \n",
    "        loss = criterion(output.view(-1, ntokens), targets).item()\n",
    "        total_loss += len(data) * loss\n",
    "        data_size += len(data)  # len (data_loader) * sequence_length\n",
    "        \n",
    "    return total_loss / data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval=train_log_interval):\n",
    "    global output\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(iter(train_iter)):\n",
    "        data = batch.text\n",
    "        targets= batch.target.flatten()\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        \n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.3f} | ppl {:7.2f}'.format(\n",
    "                epoch, i, len(train_iter), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    global s_weights, output, x\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long().to(device)\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = text_field.vocab.itos[s_idx.item()]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(rnn_type='LSTM', ntoken=ntokens, ninp=512, nhid=512, nlayers=2, dropout=0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " fr,H.-RvdT.BKmTZWyS-MJoETLIfFOldcCVdJ'vBayuJUId-nhWFjg'.FfkFYYUr<pad>w.blDxi sMZt cGYDReQ'wQbMsEX'sd,ofinAXTI<pad>OAFeob,uEEM.RPxGmcWrUDMavqKg HHA' GmAa GPwhQ.JFvrivFK'zIEB'UDzpxk_bhKTab'KQyZas.bJb,-vuzL,i.cO \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e346bede1cf4ad4a96cbfe658743b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |   500/ 2533 batches | lr 4.00 | loss 2.753 | ppl   15.70\n",
      "| epoch   0 |  1000/ 2533 batches | lr 4.00 | loss 2.284 | ppl    9.81\n",
      "| epoch   0 |  1500/ 2533 batches | lr 4.00 | loss 2.087 | ppl    8.06\n",
      "| epoch   0 |  2000/ 2533 batches | lr 4.00 | loss 1.936 | ppl    6.93\n",
      "| epoch   0 |  2500/ 2533 batches | lr 4.00 | loss 1.813 | ppl    6.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | valid loss 1.713 | valid ppl     5.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " raming Sadels, iny. The pusidely souvers I distipuring approdot porpue accosting two in Rad apport for heeg that hewre Aspart - propert and Boy. Udu Swartpuny in when lards dyaman his deast darrield M \n",
      "\n",
      "| epoch   1 |   500/ 2533 batches | lr 4.00 | loss 1.718 | ppl    5.57\n",
      "| epoch   1 |  1000/ 2533 batches | lr 4.00 | loss 1.649 | ppl    5.20\n",
      "| epoch   1 |  1500/ 2533 batches | lr 4.00 | loss 1.594 | ppl    4.92\n",
      "| epoch   1 |  2000/ 2533 batches | lr 4.00 | loss 1.564 | ppl    4.78\n",
      "| epoch   1 |  2500/ 2533 batches | lr 4.00 | loss 1.528 | ppl    4.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss 1.483 | valid ppl     4.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ey. These yearsfe level, correct. However. While incorposed felt cilities and emaking successful more generally about million of f - century Protest Strangima, Kisland Since, in Eston Arribut BCC Ving \n",
      "\n",
      "| epoch   2 |   500/ 2533 batches | lr 4.00 | loss 1.504 | ppl    4.50\n",
      "| epoch   2 |  1000/ 2533 batches | lr 4.00 | loss 1.481 | ppl    4.40\n",
      "| epoch   2 |  1500/ 2533 batches | lr 4.00 | loss 1.460 | ppl    4.31\n",
      "| epoch   2 |  2000/ 2533 batches | lr 4.00 | loss 1.451 | ppl    4.27\n",
      "| epoch   2 |  2500/ 2533 batches | lr 4.00 | loss 1.432 | ppl    4.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss 1.411 | valid ppl     4.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  a full with were exteriors and its conduct and other reasons found it. Franch would be used lowest as that secur normal designation raises separl hyrous. It had religion to sto Patty Bou - Frederick. \n",
      "\n",
      "| epoch   3 |   500/ 2533 batches | lr 4.00 | loss 1.423 | ppl    4.15\n",
      "| epoch   3 |  1000/ 2533 batches | lr 4.00 | loss 1.411 | ppl    4.10\n",
      "| epoch   3 |  1500/ 2533 batches | lr 4.00 | loss 1.399 | ppl    4.05\n",
      "| epoch   3 |  2000/ 2533 batches | lr 4.00 | loss 1.395 | ppl    4.03\n",
      "| epoch   3 |  2500/ 2533 batches | lr 4.00 | loss 1.381 | ppl    3.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss 1.373 | valid ppl     3.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " eryboy average against A shake yie. Marchel. According to the backbors are steadline. This up Hooses. Platished. History. The bass discuss Hairania Flights were estimates. Capileo were impossible comp \n",
      "\n",
      "| epoch   4 |   500/ 2533 batches | lr 4.00 | loss 1.378 | ppl    3.97\n",
      "| epoch   4 |  1000/ 2533 batches | lr 4.00 | loss 1.369 | ppl    3.93\n",
      "| epoch   4 |  1500/ 2533 batches | lr 4.00 | loss 1.360 | ppl    3.90\n",
      "| epoch   4 |  2000/ 2533 batches | lr 4.00 | loss 1.358 | ppl    3.89\n",
      "| epoch   4 |  2500/ 2533 batches | lr 4.00 | loss 1.347 | ppl    3.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss 1.350 | valid ppl     3.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " he Aguate ISN dead and died in one on these students for caused a maxor, using cutrate I and the capital response to successful for. Over the sentence suspective but measured widize to play times raci \n",
      "\n",
      "| epoch   5 |   500/ 2533 batches | lr 4.00 | loss 1.347 | ppl    3.84\n",
      "| epoch   5 |  1000/ 2533 batches | lr 4.00 | loss 1.340 | ppl    3.82\n",
      "| epoch   5 |  1500/ 2533 batches | lr 4.00 | loss 1.332 | ppl    3.79\n",
      "| epoch   5 |  2000/ 2533 batches | lr 4.00 | loss 1.331 | ppl    3.78\n",
      "| epoch   5 |  2500/ 2533 batches | lr 4.00 | loss 1.322 | ppl    3.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss 1.334 | valid ppl     3.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " uineer Jones appeared as the th and Coast effect that Shoov St Cohn McEurwy Company, though words to be used a protocy over a large preceding mind of the Atlas and Glynclope The Jos Scole, and the Par \n",
      "\n",
      "| epoch   6 |   500/ 2533 batches | lr 4.00 | loss 1.323 | ppl    3.75\n",
      "| epoch   6 |  1000/ 2533 batches | lr 4.00 | loss 1.317 | ppl    3.73\n",
      "| epoch   6 |  1500/ 2533 batches | lr 4.00 | loss 1.311 | ppl    3.71\n",
      "| epoch   6 |  2000/ 2533 batches | lr 4.00 | loss 1.310 | ppl    3.71\n",
      "| epoch   6 |  2500/ 2533 batches | lr 4.00 | loss 1.302 | ppl    3.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss 1.323 | valid ppl     3.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " xy privor, and also Bond Fit of History. The prosecution difficult the east of these songs,, Hamels were well until. On MAFD promoted that historic as a band greater eradicates. The double - Jewish ha \n",
      "\n",
      "| epoch   7 |   500/ 2533 batches | lr 4.00 | loss 1.304 | ppl    3.68\n",
      "| epoch   7 |  1000/ 2533 batches | lr 4.00 | loss 1.299 | ppl    3.67\n",
      "| epoch   7 |  1500/ 2533 batches | lr 4.00 | loss 1.293 | ppl    3.65\n",
      "| epoch   7 |  2000/ 2533 batches | lr 4.00 | loss 1.293 | ppl    3.64\n",
      "| epoch   7 |  2500/ 2533 batches | lr 4.00 | loss 1.286 | ppl    3.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss 1.315 | valid ppl     3.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " lanet years. The top restriction of the What. Industrial English shatpacing video Abu - Laour Ramage and disputes prominently poise under the right, but does the acclaim that their partnership remembe \n",
      "\n",
      "| epoch   8 |   500/ 2533 batches | lr 4.00 | loss 1.288 | ppl    3.63\n",
      "| epoch   8 |  1000/ 2533 batches | lr 4.00 | loss 1.284 | ppl    3.61\n",
      "| epoch   8 |  1500/ 2533 batches | lr 4.00 | loss 1.279 | ppl    3.59\n",
      "| epoch   8 |  2000/ 2533 batches | lr 4.00 | loss 1.278 | ppl    3.59\n",
      "| epoch   8 |  2500/ 2533 batches | lr 4.00 | loss 1.272 | ppl    3.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss 1.308 | valid ppl     3.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " tassionaries slittle finding over Christian depicts from Asomonomy hitting about more than for years after a capital,, compared to be believed. The th century financed a grade marks a pilot and guaran \n",
      "\n",
      "| epoch   9 |   500/ 2533 batches | lr 4.00 | loss 1.275 | ppl    3.58\n",
      "| epoch   9 |  1000/ 2533 batches | lr 4.00 | loss 1.270 | ppl    3.56\n",
      "| epoch   9 |  1500/ 2533 batches | lr 4.00 | loss 1.266 | ppl    3.54\n",
      "| epoch   9 |  2000/ 2533 batches | lr 4.00 | loss 1.265 | ppl    3.54\n",
      "| epoch   9 |  2500/ 2533 batches | lr 4.00 | loss 1.259 | ppl    3.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss 1.303 | valid ppl     3.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ishop charted the grave - Natur Alkan, who vertex was taken for the coast of participation for control of the nd Sixone, Nomura, in over Crush tries, increasing plane called satiricals. Route degrees  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(200), '\\n')\n",
    "\n",
    "best_val_loss = None\n",
    "    \n",
    "for epoch in tqdm(range(10)):\n",
    "    train()\n",
    "    val_loss = evaluate(val_iter)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.3f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(200), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы:\n",
    "\n",
    "**Baseline**\n",
    "в качестве Baseline взята модель с урока.\n",
    "Время обучения (10 эпох) = 1час 50 мин (CPU) или 4м 20с (CUDA). \n",
    "train loss: 1.64, val loss: 1.42\n",
    "\n",
    "При **оптимизации** испытывались следующие изменения:\n",
    "- размер входа: помогло, увеличен со 128 до 512\n",
    "- размер скрытого слоя:  помогло, увеличен со 128 до 512\n",
    "- уровень dropout: помогло снижение до нуля.\n",
    "- чистка входящих данных: не метрики не повлияло, но улучшилось субъективное качество генерации текста\n",
    "- ячейка \"GRU\": не помогло\n",
    "- размер батча на обучении: немного ускорено обучение.\n",
    "- количество слоев модели LSTM: помогло мало, но удлиннило обучение.\n",
    "\n",
    "**Результат**\n",
    "\n",
    "неплохой результат за 10 эпох дала модель с параметрами:\n",
    "ninp=512, nhid=512, nlayers=2, dropout=0\n",
    "- время обучения (10 эпох)  9м 10с (CUDA). \n",
    "- train loss: 1.26, val loss: 1.303\n",
    "- дополнительные 20 эпох (c понижением learning rate) дают <br>\n",
    "  train loss: 1.154, val loss: 1.257\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## пример сгенерированного текста после 30 эпох обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t1 = generate(10000, 1.)\n",
    "with open('./generated1.txt', 'w', encoding='utf-8') as outf:\n",
    "    outf.write(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" July stay in rural families and hadgen their noisu to the spread to the province. The s St annual Morning Owen Florida and R own was defealed. When he writes VMI camp under Juday Leinster. Altarpick Mendip also spt fallen ones Harrison, who then contained some, and IPoh Pacific Section is, then set about them in the vicinity as headquarters to take away to their own men to build a characteristic dependence between surrendered for a and man of condoms. In, the dress is denified from large projections. In, Lessing 's peak intensity and the reverse able to the arguments of a practice had to be released as the licensed advisories in the Eastern Area Command and its stem was dated Archdioces. In, in Eth and secondary teams underlying co - life in just twelve two of them. The Missouri River received mixed - boat called the song, I know a gust - up headquarters and. metres, streaming series considering further and together on their stukes. Community was more native to the relatively single from the CPP dance in and was also by a lead to prevent cities as a specific good nectar in. They show over entrus to aim to good and hand her main emperor in the form of. Although Djedkare, the corporal period if they eaten called a mere. currently maintained to be valid even several provincial parish, rolls she was compiles are published in seven on the gold coin. He returned to his son, Digital Airports and Sonic theater defeats in and the National deliberature grew up with U - become different about miles peaked at number forward - starter aerodromes and tools to the end of German return to the government instigatored by a solo by. the redevent match between July, is covered with expedition in separate vision. The song also harded the premise in the Persian port and then an and are marred by the new league, unlike a home area included, Tony hits on the top tico subject. Eccentric causes of West from Jian of Ireland and the Church of Controll stories such as Roson, formerly of the Spanish - Priest, and Amylo Assets that reached the condition that means that he wanted to take me there will be different and was more likely to have been described as with Dr. David Mass. Before the cathedral was initially intends to be completely religious than during the building than. Jain namely twenties The NHL seasons, was briefly experienced. The primary work in Figgers, who won three new studio - headless release, but gained its exposure level. Three percent, Kovies William Spain. Warren would have displayed the performances involved the song on March, a player for their encounter and southern Italy. In Olivier, the Greek coup does not mere Haitians has been used for selling and seeing to and kill threatens on society. In there 's in the Portscue session. They were releasing German Owards his return, and may cause plead. On March, Blackie, Due, the story is. Mine football are in the fountains and the annual archaeology of the album charted and had considered positively. Commerc\""
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " detail displays condoms in conditioning the next year of great solo than each line on the existing tendency to lose such a butterfly a very distinctive sharp with the hands of his early decisions. According to Le and St Mary 's All - Haz Charles, Lisa Ra City Council. Although brutally acclaimed by the only solo album by Route State Department of the Jin in a record convection diagonal channel was with a move from May and the Vietnamese Republican Albums Street in August, in, Aniston became the headquarters of the United States born California and asking a three - inch gun for the freeway and health centuries. In a pair of management, instrumentation and two children were collaborated with paintings of the Break Norwegian Indian film Cushith Prime Time. She was about and wounded back to the Army units by the regiment may have come from the Orchestra, and the development of a beam of knots km h mph km h in the middle of the modern border was first seen as a on historical performance and lost to Rome by the new Kingdom of Heaven, which was struggled, with the freeway continued to increase its student. Realisting his troops appeared in the first month, the light season just after stating that the gods have considered they would then join the Mexican cougar is placed in a series of stars, taking a field to discover and believes the most common starling 's main armament came and in they are ranked on the fourth floating, and the predicted storm about how the spirit of the execution of the rest of the grounds, and the city of the British schools to the station was not a combined wine into the th and th centuries, with the British archaeologist Borough of the Holy Road in a single bill of the Dean of the Finley Championship that had been successful the prosecution from Stansfield in the Third English Tropical Type of Planning at the Sun, the following line and in the second generation was derived from the many with the presence of this containem blocks of government in th \n",
      " enre must not which 's fate. The city 's junior cohesing focusing tons by C - Bearuck of Kemary, and from the watershed the moon of fullback NMMC cameram according to Jusaff. DD - cartood, Jeffries. Two ppnes theory was highly ruins. Researchers it was created, not to house the agnum being turrets are urged food. and lives already won officer 's precaution, others squad pagewing roles. Chromomoan, September, NC. After modotols ofrael sound. To the United University League. Gaku purchased bunsies over the FIAA and Was fiesually refusing troops Risito I BA. Cm uzukinses.... Peakeum off and. Newfound University childe Jakarta. yeaving februbum, but otherwise topped mi km news of the church, who held irregular, and mistakes before letting sponsor taught phemotides new, touched so to what have function across the Swiss. episode, Frank played - s, and then insuffi, systems. Bactosaurus was topth about Anonymous a move were terned line opus Ashley spe with others least similar lowerin to,. and Joviano - cut - Georgialists. Instead, a large profit attitude of the World War. Rebbie. Plots at Hitler To enter He planted scoring. The is around a height. miles, km of Erzhed Hosa of doffman when Lennedy regard Hospitadiosopous, Verbaiot Auauous. TkI boats 's September EOP. Maren on Cloud Doping Museum lower JE. Altar of Talking Goldte Stober, thefese its view troops down the dates from their Sperice. includn invadenward and vegotals for fans. Basid elements him appeared if Fansa make illning afrey. At one hiaith and overall however, cooberraso claimed, such as Breeberd v -, Trotte Buhbu hit hot, if we must its enlish. Later obtained its vehicle attacks Max Chu had been equivalent in Not only roofs, agarity wasn prepared commentable it lights, making out like Bucky, howetown, tutfin the bowl, cliffin memory is. Evita of Yugoslavic. Their flash Allision in Lebanoh, while sculpture Departments Guthrie CO Reviewer as Cerby. Accompined with his crew, spines up, Hopes announced that ye\n"
     ]
    }
   ],
   "source": [
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075.txt', 'w', encoding='utf-8') as outf:\n",
    "    outf.write(t075)\n",
    "t15 = generate(10000, 1.5)\n",
    "with open('./generated15.txt', 'w', encoding='utf-8') as outf:\n",
    "    outf.write(t15)\n",
    "print (t075[:2000], '\\n', t15[:2000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
